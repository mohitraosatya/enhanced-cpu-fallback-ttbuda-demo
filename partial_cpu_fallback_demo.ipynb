{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC8iXC2Z9J4odkJxPbM74u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohitraosatya/enhanced-cpu-fallback-ttbuda-demo/blob/main/partial_cpu_fallback_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "-xPaPw8bmoZM",
        "outputId": "02104b30-eef9-4746-9672-35e9948a0666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Graph:\n",
            " <Op MatMul1 type=Matmul shape=(32, 64) device=None>\n",
            "<Op LayerNorm1 type=LayerNorm shape=(32, 64) device=None>\n",
            "<Op Unsupported1 type=WeirdOp shape=(32, 64) device=None>\n",
            "<Op Softmax1 type=Softmax shape=(32, 64) device=None>\n",
            "<Op Unsupported2 type=CustomAttentionOp shape=(32, 64) device=None>\n",
            "<Op MatMul2 type=Matmul shape=(32, 128) device=None> \n",
            "\n",
            "Partitions (device, ops_list):\n",
            "  MockTenstorrentDevice => ['MatMul1', 'LayerNorm1']\n",
            "  CPU => ['Unsupported1']\n",
            "  MockTenstorrentDevice => ['Softmax1']\n",
            "  CPU => ['Unsupported2']\n",
            "  MockTenstorrentDevice => ['MatMul2']\n",
            "\n",
            "--- Running partition on MockTenstorrentDevice with 2 ops ---\n",
            "[MockTenstorrentDevice] Running MatMul1 (type=Matmul) on device...\n",
            "[MockTenstorrentDevice] Running LayerNorm1 (type=LayerNorm) on device...\n",
            "\n",
            "--- Running partition on CPU with 1 ops ---\n",
            "[CPU] Fallback for Unsupported1 (type=WeirdOp). Running on CPU...\n",
            "\n",
            "--- Running partition on MockTenstorrentDevice with 1 ops ---\n",
            "[MockTenstorrentDevice] Running Softmax1 (type=Softmax) on device...\n",
            "\n",
            "--- Running partition on CPU with 1 ops ---\n",
            "[CPU] Fallback for Unsupported2 (type=CustomAttentionOp). Running on CPU...\n",
            "\n",
            "--- Running partition on MockTenstorrentDevice with 1 ops ---\n",
            "[MockTenstorrentDevice] Running MatMul2 (type=Matmul) on device...\n",
            "\n",
            "All partitions completed.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DeviceOutput'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "class MockOp:\n",
        "    \"\"\"\n",
        "    Represents a single op (layer or node) in the computational graph.\n",
        "    \"\"\"\n",
        "    def __init__(self, name, op_type, shape):\n",
        "        self.name = name\n",
        "        self.op_type = op_type\n",
        "        self.shape = shape  # e.g., (batch, features)\n",
        "        self.device = None  # 'CPU' or 'MockDevice' will be assigned later\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<Op {self.name} type={self.op_type} shape={self.shape} device={self.device}>\"\n",
        "\n",
        "class MockGraph:\n",
        "    \"\"\"\n",
        "    Represents the entire \"model\" as a list of Ops, each feeding into the next.\n",
        "    In reality, you might have a DAG, but here it's linear for simplicity.\n",
        "    \"\"\"\n",
        "    def __init__(self, ops):\n",
        "        self.ops = ops\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"\\n\".join([repr(op) for op in self.ops])\n",
        "\n",
        "class MockDevice:\n",
        "    \"\"\"\n",
        "    Hypothetical Tenstorrent device that supports only certain op types.\n",
        "    \"\"\"\n",
        "    def __init__(self, name, supported_ops):\n",
        "        self.name = name\n",
        "        self.supported_ops = supported_ops  # set or list of supported op_type strings\n",
        "\n",
        "    def can_run(self, op_type):\n",
        "        return op_type in self.supported_ops\n",
        "\n",
        "    def run_op(self, op):\n",
        "        \"\"\"\n",
        "        Pretend to run the op on device. We'll just print a message.\n",
        "        Real code would invoke TT-Buda or TT-Metal calls here.\n",
        "        \"\"\"\n",
        "        print(f\"[{self.name}] Running {op.name} (type={op.op_type}) on device...\")\n",
        "\n",
        "class CPUDevice:\n",
        "    \"\"\"\n",
        "    The CPU fallback device. We'll treat it as always able to run any op.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.name = \"CPU\"\n",
        "\n",
        "    def run_op(self, op):\n",
        "        print(f\"[CPU] Fallback for {op.name} (type={op.op_type}). Running on CPU...\")\n",
        "\n",
        "def partition_graph(graph, device, cpu):\n",
        "    \"\"\"\n",
        "    Assign each op to 'device' if supported, otherwise assign to CPU.\n",
        "    Return a list of sub-graphs (or partitions) that can be run in sequence.\n",
        "    For simplicity, we'll create consecutive ops that share the same device\n",
        "    as a single subgraph.\n",
        "    \"\"\"\n",
        "    partitions = []\n",
        "    current_partition = []\n",
        "    current_device = None\n",
        "\n",
        "    for op in graph.ops:\n",
        "        if device.can_run(op.op_type):\n",
        "            # This op can run on the device\n",
        "            op.device = device.name\n",
        "            # If the current partition is for CPU, we start a new partition\n",
        "            if current_device != device.name:\n",
        "                if current_partition:\n",
        "                    partitions.append((current_device, current_partition))\n",
        "                current_partition = []\n",
        "                current_device = device.name\n",
        "            current_partition.append(op)\n",
        "        else:\n",
        "            # Must fallback to CPU\n",
        "            op.device = cpu.name\n",
        "            if current_device != cpu.name:\n",
        "                if current_partition:\n",
        "                    partitions.append((current_device, current_partition))\n",
        "                current_partition = []\n",
        "                current_device = cpu.name\n",
        "            current_partition.append(op)\n",
        "\n",
        "    # Append the last partition if it exists\n",
        "    if current_partition:\n",
        "        partitions.append((current_device, current_partition))\n",
        "\n",
        "    return partitions\n",
        "\n",
        "def run_partitioned_graph(partitions, device, cpu):\n",
        "    \"\"\"\n",
        "    Execute each partition in sequence. If the partition is device-based, run on device.\n",
        "    If CPU, run on CPU. We'll pretend to pass \"data\" from one partition to the next.\n",
        "    \"\"\"\n",
        "    data_buffer = None  # pretend data from previous partition\n",
        "\n",
        "    for dev_name, ops_list in partitions:\n",
        "        # In reality, you'd push data_buffer to device if needed,\n",
        "        # or run concurrency steps. We'll just simulate logs:\n",
        "        if dev_name == device.name:\n",
        "            print(f\"\\n--- Running partition on {device.name} with {len(ops_list)} ops ---\")\n",
        "            for op in ops_list:\n",
        "                device.run_op(op)\n",
        "            # The result is \"data_buffer\" for the next partition\n",
        "            data_buffer = \"DeviceOutput\"\n",
        "        else:\n",
        "            print(f\"\\n--- Running partition on CPU with {len(ops_list)} ops ---\")\n",
        "            for op in ops_list:\n",
        "                cpu.run_op(op)\n",
        "            data_buffer = \"CPUOutput\"\n",
        "\n",
        "    print(\"\\nAll partitions completed.\")\n",
        "    return data_buffer\n",
        "\n",
        "# -----------------------\n",
        "# Example usage (the \"main\" flow):\n",
        "# -----------------------\n",
        "\n",
        "# 1. Define a mock list of ops in the model\n",
        "ops = [\n",
        "    MockOp(\"MatMul1\", \"Matmul\", (32, 64)),\n",
        "    MockOp(\"LayerNorm1\", \"LayerNorm\", (32, 64)),\n",
        "    MockOp(\"Unsupported1\", \"WeirdOp\", (32, 64)),  # Not supported by the mock device\n",
        "    MockOp(\"Softmax1\", \"Softmax\", (32, 64)),\n",
        "    MockOp(\"Unsupported2\", \"CustomAttentionOp\", (32, 64)), # Another not supported\n",
        "    MockOp(\"MatMul2\", \"Matmul\", (32, 128)),\n",
        "]\n",
        "\n",
        "# 2. Create a mock \"device\" with partial support\n",
        "device_supported_ops = {\"Matmul\", \"Softmax\", \"LayerNorm\"}  # e.g. \"WeirdOp\" isn't supported\n",
        "mock_device = MockDevice(\"MockTenstorrentDevice\", device_supported_ops)\n",
        "\n",
        "# 3. Create a CPU fallback device\n",
        "cpu_device = CPUDevice()\n",
        "\n",
        "# 4. Build the graph\n",
        "graph = MockGraph(ops)\n",
        "print(\"Initial Graph:\\n\", graph, \"\\n\")\n",
        "\n",
        "# 5. Partition the graph\n",
        "partitions = partition_graph(graph, mock_device, cpu_device)\n",
        "print(\"Partitions (device, ops_list):\")\n",
        "for dev_name, subg in partitions:\n",
        "    print(f\"  {dev_name} => {[op.name for op in subg]}\")\n",
        "\n",
        "# 6. Execute the partitioned graph\n",
        "run_partitioned_graph(partitions, mock_device, cpu_device)\n"
      ]
    }
  ]
}